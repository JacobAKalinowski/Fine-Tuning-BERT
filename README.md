# Fine-Tuning-BERT

[BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) is a very popular model in natural language processing (NLP).  It is based on the transformer architecture, and there are several pretrained BERT models that can be used.  These models can be fine tuned on smaller datasets for less training time since they have been trained on larger datasets.  

I used the sst2 dataset from huggingface and the bert-base-uncased model.  I trained the model on 2 epochs using a NVIDIA Tesla T4 on Google Cloud Platform.  

I did this project to get practice using a pretrained model like BERT, and I followed Huggingface documentation in order to do this.
