# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13_m3A3RBH7qHiXkLtN5coUbeHhV_UDWV
"""

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from datasets import load_dataset, load_metric

#Loading Tokenizer and sst2 dataset from Huggingface

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
dataset = load_dataset("glue", "sst2")

dataset

train_data = dataset["train"]
validation_data = dataset["validation"]
test_data = dataset["test"]

def tokenize_help(data):
  return tokenizer(data["sentence"], truncation = True)

tokenized_train_data = train_data.map(tokenize_help, batched = True)
tokenized_validation_data = validation_data.map(tokenize_help, batched = True)
tokenized_test_data = test_data.map(tokenize_help, batched = True)

print(tokenized_train_data.features)

tokenized_train_data = tokenized_train_data.remove_columns(["sentence", "idx"])
tokenized_train_data = tokenized_train_data.rename_column("label", "labels")

tokenized_validation_data = tokenized_validation_data.remove_columns(["sentence", "idx"])
tokenized_validation_data = tokenized_validation_data.rename_column("label", "labels")

tokenized_test_data = tokenized_test_data.remove_columns(["sentence", "idx"])
tokenized_test_data = tokenized_test_data.rename_column("label", "labels")

tokenized_train_data.set_format("torch")
tokenized_validation_data.set_format("torch")
tokenized_test_data.set_format("torch")

from transformers import DataCollatorWithPadding
from torch.utils.data import DataLoader

train_iterator = DataLoader(
    tokenized_train_data, shuffle = True, batch_size = 4, collate_fn = DataCollatorWithPadding(tokenizer = tokenizer)
)

validation_iterator = DataLoader(
    tokenized_validation_data, shuffle = True, batch_size = 4, collate_fn = DataCollatorWithPadding(tokenizer = tokenizer)
)

#See what the data looks like in each batch
for batch in train_iterator:
    break
{k: v.shape for k, v in batch.items()}

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device

model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2)
model.to(device)

model.load_state_dict(torch.load("fine_tuned_bert"))

from torch import optim
from transformers import get_scheduler


epochs = 2
optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-5)
steps = epochs * len(train_iterator)
scheduler = get_scheduler(
    "linear", 
    optimizer = 
    optimizer, 
    num_warmup_steps = 0, 
    num_training_steps = steps
)

from tqdm.auto import tqdm

progress = tqdm(range(steps))

for epoch in range(epochs):
  for batch in train_iterator:
      batch = {k: v.to(device) for k, v in batch.items()}
      output = model(**batch)

      loss = output.loss
      loss.backward()

      optimizer.step()
      scheduler.step()
      optimizer.zero_grad()
      progress.update(1)

metric = load_metric("glue", "sst2")
model.eval
for batch in validation_iterator:
    batch = {k: v.to(device) for k, v in batch.items()}
    #Evaluating Model
    with torch.no_grad():
        out = model(**batch)
        
    logits = out.logits
    predictions = torch.argmax(logits, dim = -1)
    metric.add_batch(predictions = predictions, references = batch["labels"])
    
metric.compute()

from sklearn.metrics import classification_report
len(predictions)
#print(classification_report(validation_data["label"], predictions))

torch.save(model.state_dict(), "fine_tuned_bert")

